import re
import pandas as pd
import os
import glob
import sys

def format_text(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢ã™ã‚‹ã€‚ã€Œå·ã€ãƒ¬ãƒ™ãƒ«ï¼ˆ(1), (2), ï¼ˆ1ï¼‰ãªã©ï¼‰ã¨
    ã‚«ã‚¿ã‚«ãƒŠï¼ˆã‚¢ã€ã‚¤ã€ã‚¦ãªã©ï¼‰ã§å§‹ã¾ã‚‹è¡Œã®å‰ã®æ”¹è¡Œã‚’å‰Šé™¤ã™ã‚‹ã€‚
    
    Parameters:
    - text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
    - formatted_text: æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    # è¡Œã”ã¨ã«åˆ†å‰²
    lines = text.split('\n')
    
    # æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼ç´y
    formatted_lines = []
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        
        # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
        if not line_stripped:
            continue
        
        # ã€Œå·ã€ãƒ‘ã‚¿ãƒ¼ãƒ³: (1), (2), ï¼ˆ1ï¼‰, ï¼ˆ2ï¼‰ ãªã©ï¼ˆæ‹¬å¼§+æ•°å­—ï¼‰ã§å§‹ã¾ã‚‹è¡Œ
        is_gou = re.match(r'^[ï¼ˆ\(]\d+[ï¼‰\)]', line_stripped)
        
        # ã‚«ã‚¿ã‚«ãƒŠãƒ‘ã‚¿ãƒ¼ãƒ³: ã‚¢ã€ã‚¤ã€ã‚¦ã€ã‚¨ã€ã‚ª ãªã©ã§å§‹ã¾ã‚‹è¡Œ
        is_katakana = re.match(r'^[ã‚¡-ãƒ¶ãƒ¼]+[\sã€€]', line_stripped) or re.match(r'^[ã‚¡-ãƒ¶ãƒ¼]+$', line_stripped) or re.match(r'^[ã‚¡-ãƒ¶ãƒ¼]+[^ã‚¡-ãƒ¶ãƒ¼]', line_stripped)
        
        if (is_gou or is_katakana) and formatted_lines:
            # å·ã¾ãŸã¯ã‚«ã‚¿ã‚«ãƒŠã®å ´åˆã¯æ”¹è¡Œã›ãšã€ç›´å‰ã®è¡Œã«é€£çµ
            # å‰ã®è¡Œã®æœ«å°¾ã«ã€Œã€‚ã€ãŒãªã„å ´åˆã¯è¿½åŠ 
            if not formatted_lines[-1].endswith('ã€‚'):
                formatted_lines[-1] += 'ã€‚'
            formatted_lines[-1] += line_stripped
        else:
            # ãã‚Œä»¥å¤–ï¼ˆæ¡ã€é …ãªã©ï¼‰ã¯æ”¹è¡Œã‚’ä¿æŒ
            formatted_lines.append(line_stripped)
    
    # æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    formatted_text = '\n'.join(formatted_lines)
    
    return formatted_text

def extract_metadata_from_filename(filename):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è‡ªæ²»ä½“åã¨åŒºåˆ†ã‚’æŠ½å‡ºã™ã‚‹
    ä¾‹: "èŠ³è³€ç”º_Haga_Town_Ordinance_PDF.txt" -> ("èŠ³è³€ç”º", "æ¡ä¾‹")
        "Haga_Town_Ordinance_PDF.txt" -> ("Haga Town", "æ¡ä¾‹")
        "Haga_Town_Regulation_HTML.txt" -> ("Haga Town", "æ–½è¡Œè¦å‰‡")
    
    Parameters:
    - filename: ãƒ•ã‚¡ã‚¤ãƒ«å
    
    Returns:
    - jichitai: è‡ªæ²»ä½“å
    - kubun: åŒºåˆ†
    """
    # æ‹¡å¼µå­ã‚’é™¤å»
    name = os.path.splitext(filename)[0]
    
    # "_PDF", "_HTML", "_Ordinance", "_Regulation" ãªã©ã®ãƒã‚¤ã‚ºã‚’é™¤å»
    # åŒºåˆ†ã‚’åˆ¤å®š
    if 'Ordinance' in name:
        kubun = 'æ¡ä¾‹'
        name = name.replace('_Ordinance', '')
    elif 'Regulation' in name:
        kubun = 'æ–½è¡Œè¦å‰‡'
        name = name.replace('_Regulation', '')
    else:
        kubun = 'ä¸æ˜'
    
    # PDFã‚„HTMLã®ãƒã‚¤ã‚ºã‚’é™¤å»
    name = name.replace('_PDF', '').replace('_HTML', '')
    
    # æ—¥æœ¬èªã¨è‹±èªãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã€æ—¥æœ¬èªã‚’å„ªå…ˆ
    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã§åˆ†å‰²
    parts = name.split('_')
    
    # æ—¥æœ¬èªã‚’å«ã‚€éƒ¨åˆ†ã‚’æ¢ã™
    japanese_parts = []
    for part in parts:
        # æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if re.search(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¥]', part):
            japanese_parts.append(part)
    
    # æ—¥æœ¬èªãŒã‚ã‚‹å ´åˆã¯æ—¥æœ¬èªã‚’å„ªå…ˆã€ãªã‘ã‚Œã°è‹±èªéƒ¨åˆ†ã‚’ä½¿ç”¨
    if japanese_parts:
        jichitai = '_'.join(japanese_parts)
    else:
        # è‹±èªã®å ´åˆã¯ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        jichitai = name.replace('_', ' ').strip()
    
    return jichitai, kubun

def extract_year_from_directory(directory):
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰åˆ¶å®šå¹´ã‚’æŠ½å‡ºã™ã‚‹
    ä¾‹: "out_2022" -> "2022"
        "out_2023" -> "2023"
    
    Parameters:
    - directory: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
    
    Returns:
    - year: åˆ¶å®šå¹´ï¼ˆæ–‡å­—åˆ—ï¼‰ã€æŠ½å‡ºã§ããªã„å ´åˆã¯None
    """
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰4æ¡ã®æ•°å­—ã‚’æŠ½å‡º
    match = re.search(r'(\d{4})', directory)
    if match:
        return match.group(1)
    return None

def process_multiple_files(directories, output_csv):
    """
    è¤‡æ•°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ•´å½¢ã—ã¦CSVã«ã¾ã¨ã‚ã‚‹
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰åˆ¶å®šå¹´ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹
    
    Parameters:
    - directories: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    - output_csv: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    
    Returns:
    - success_count: æˆåŠŸã—ãŸä»¶æ•°
    - error_count: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸä»¶æ•°
    - duplicate_count: é‡è¤‡ã—ãŸä»¶æ•°
    """
    all_data = []
    success_count = 0
    error_count = 0
    duplicate_count = 0
    
    # æ—¢å­˜ã®CSVã‚’èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    existing_df = None
    if os.path.exists(output_csv):
        try:
            existing_df = pd.read_csv(output_csv)
            if len(existing_df) > 0:
                print(f"æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(existing_df)} è¡Œ")
            else:
                print("æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ç©ºã§ã™ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
                existing_df = None
        except (pd.errors.EmptyDataError, pd.errors.ParserError) as e:
            print(f"æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            print("æ–°ã—ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
            existing_df = None
    
    # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    for directory in directories:
        if not os.path.exists(directory):
            print(f"âš ï¸  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {directory}")
            continue
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰åˆ¶å®šå¹´ã‚’æŠ½å‡º
        seiteinen = extract_year_from_directory(directory)
        if not seiteinen:
            print(f"âš ï¸  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰åˆ¶å®šå¹´ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“: {directory}")
            continue
        
        print(f"\nğŸ“ å‡¦ç†ä¸­: {directory} (åˆ¶å®šå¹´: {seiteinen})")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        txt_files = glob.glob(os.path.join(directory, '*.txt'))
        print(f"   {len(txt_files)} å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        
        for txt_file in txt_files:
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                with open(txt_file, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢
                formatted_text = format_text(text)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                filename = os.path.basename(txt_file)
                jichitai, kubun = extract_metadata_from_filename(filename)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ã®CSVãŠã‚ˆã³ä»Šå›è¿½åŠ ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ï¼‰
                is_duplicate = False
                
                # æ—¢å­˜ã®CSVã§ãƒã‚§ãƒƒã‚¯
                if existing_df is not None:
                    duplicate_rows = existing_df[(existing_df['è‡ªæ²»ä½“'] == jichitai) & 
                                                (existing_df['åˆ¶å®šå¹´'] == seiteinen) & 
                                                (existing_df['åŒºåˆ†'] == kubun)]
                    is_duplicate = len(duplicate_rows) > 0
                
                # ä»Šå›è¿½åŠ ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿å†…ã§ã‚‚ãƒã‚§ãƒƒã‚¯
                if not is_duplicate and all_data:
                    for existing_row in all_data:
                        if (existing_row['è‡ªæ²»ä½“'] == jichitai and 
                            existing_row['åˆ¶å®šå¹´'] == seiteinen and 
                            existing_row['åŒºåˆ†'] == kubun):
                            is_duplicate = True
                            break
                
                if is_duplicate:
                    print(f"   âš ï¸  é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {filename} ({jichitai}, {seiteinen}, {kubun})")
                    duplicate_count += 1
                    continue  # é‡è¤‡ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                row_data = {
                    'æœ¬æ–‡': formatted_text,
                    'åˆ¶å®šå¹´': seiteinen,
                    'è‡ªæ²»ä½“': jichitai,
                    'åŒºåˆ†': kubun
                }
                all_data.append(row_data)
                
                print(f"   âœ“ å‡¦ç†å®Œäº†: {filename} ({jichitai}, {seiteinen}, {kubun})")
                success_count += 1
            
            except Exception as e:
                print(f"   âœ— ã‚¨ãƒ©ãƒ¼: {filename} - {str(e)}")
                error_count += 1
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    if all_data:
        new_df = pd.DataFrame(all_data)
        
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
        if existing_df is not None:
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = new_df
        
        # CSVã«ä¿å­˜
        final_df.to_csv(output_csv, index=False, encoding='utf-8')
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å‡¦ç†å®Œäº†")
        print(f"{'='*60}")
        print(f"âœ“ æˆåŠŸ: {success_count} ä»¶")
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {error_count} ä»¶")
        print(f"âš ï¸  é‡è¤‡: {duplicate_count} ä»¶")
        print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
        print(f"ğŸ“ˆ CSVã®ç·è¡Œæ•°: {len(final_df)} è¡Œ")
        print(f"{'='*60}")
    else:
        print("\nâš ï¸  å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return success_count, error_count, duplicate_count

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*60)
    print("è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•´å½¢ãƒ„ãƒ¼ãƒ« (CUIç‰ˆ)")
    print("="*60)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®š
    output_csv = 'main2.6.csv'
    
    # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å¹´åº¦åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º
    # out_YYYYå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    all_dirs = [d for d in os.listdir('.') if os.path.isdir(d)]
    year_dirs = [d for d in all_dirs if re.match(r'out_\d{4}', d)]
    
    if not year_dirs:
        print("âš ï¸  ã‚¨ãƒ©ãƒ¼: å‡¦ç†å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   'out_YYYY' å½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¡¨ç¤º
    print(f"\næ¤œå‡ºã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:")
    for directory in sorted(year_dirs):
        year = extract_year_from_directory(directory)
        txt_count = len(glob.glob(os.path.join(directory, '*.txt')))
        print(f"  - {directory} (åˆ¶å®šå¹´: {year}, ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {txt_count})")
    
    # ç¢ºèª
    print(f"\nå‡ºåŠ›å…ˆ: {output_csv}")
    response = input("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã‹? [y/N]: ").strip().lower()
    
    if response not in ['y', 'yes']:
        print("å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        sys.exit(0)
    
    # å‡¦ç†ã‚’å®Ÿè¡Œ
    print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n")
    success_count, error_count, duplicate_count = process_multiple_files(year_dirs, output_csv)
    
    # çµ‚äº†
    if error_count > 0:
        sys.exit(1)
    else:
        sys.exit(0)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\näºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)