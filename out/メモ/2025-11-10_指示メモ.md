# 指示

## 使用するファイル
- `coding_munic..v4.xlsx`: KHCoderの出力ファイルの一つ. 各自治体条例と施行規則に含まれるコーディングの数をまとめたもの. 基礎ファイルとして使用. 
- `munic._coding.v.4.csv`: KHCoderの出力ファイルの一つ (40,158行). 基礎ファイルの詳細データ. 各段落にどのコードが含まれるのかを記載している. 
  - カラム構成: h1, h2, h3, h4, h5 (文書ID), dan (段落番号), 各種コーディング項目 (33項目)
  - h5は文書の一意識別子で、main4.3.csvの行番号に対応
- `main4.3.csv`: KHCoderに入力した分析対象ファイル (40,158行). このファイルの各行が詳細データのh5カラムに対応している
  - カラム構成: 本文, 制定年, 自治体, 自治体_eng, 区分
  - 415自治体の条例・規則本文を段落単位で格納
- `result_solar_rule_v1.1.csv`: 統計分析結果ファイル (314自治体)
  - 各自治体の規制タイプ、厳格度スコア、プロセス重視度などの分析指標を含む
- `clause-viewer/data-integrated.json`: clause-viewerで使用するデータファイル. 解析結果を可視化するために使用する
  - 現在は53自治体のみのデータで古い


## 目標
現状の`clause-viewer/data-integrated.json`ファイルはデータが不足している. その理由は, 作成に使用したデータが古いものであったためである. 今回, 使用するファイルとして指定したファイルは, いずれも最新のものである. これらを使用して, `clause-viewer/data-integrated.json`ファイルを最新のものに更新することが目標である. 

## ファイル間の関係性
```
main4.3.csv (40,158行)
  ├─ 各行に段落本文と自治体情報
  └─ 行番号 = munic._coding.v.4.csvのh5

munic._coding.v.4.csv (40,158行)
  ├─ h5カラムでmain4.3.csvと紐付け
  └─ 各段落にどのコード(33種類)が含まれるかを01で表現

result_solar_rule_v1.1.csv (314自治体)
  └─ 自治体ごとの統計分析結果 (規制タイプ、厳格度スコアなど)

↓ 統合

data-integrated.json
  ├─ municipalities: 自治体リスト
  ├─ coding_types: コーディング種別リスト
  ├─ municipality_info: 各自治体の分析結果
  ├─ paragraphs: 段落ごとのコーディング詳細
  └─ statistics: 全体統計情報
```

## 手順

### 1. 現状確認
- [x] 各ファイルの構造とデータ量を確認
  - main4.3.csv: 40,158行 (415自治体)
  - munic._coding.v.4.csv: 40,158行 (h5=1〜40157)
  - result_solar_rule_v1.1.csv: 314自治体
  - data-integrated.json: 現在53自治体のみ

### 2. データ統合スクリプトの作成
既存の`clause-viewer/generate_integrated_data.py`を参考に、新しいスクリプトを作成する:

#### 2.1 必要な処理
1. **main4.3.csvの読み込み**
   - 段落本文、自治体名、制定年などを取得
   
2. **munic._coding.v.4.csvの読み込み**
   - h5をキーとして、各段落のコーディング情報を取得
   - 33種類のコーディング項目を配列化

3. **result_solar_rule_v1.1.csvの読み込み**
   - 自治体ごとの分析結果を辞書化

4. **データの統合**
   - main4.3.csvとmunic._coding.v.4.csvをh5で結合
   - 段落ごとに以下の情報を含むオブジェクトを作成:
     ```json
     {
       "h5": 整数,
       "municipality": "自治体名",
       "year": "制定年",
       "text": "段落本文",
       "codes": ["*CODE1", "*CODE2", ...],
       "dan": 段落番号
     }
     ```

5. **最終JSON構造の作成**
   ```json
   {
     "municipalities": ["自治体1", "自治体2", ...],
     "coding_types": ["*CLAUSE_...", ...],
     "municipality_info": {
       "自治体名": {
         "ケース数": int,
         "規制タイプ": str,
         "厳格度スコア": float,
         ...
       }
     },
     "paragraphs": [...],
     "statistics": {...},
     "metadata": {...}
   }
   ```

### 3. スクリプトの実行
```bash
cd /home/ubuntu/cur/isep/clause-viewer
python3 generate_integrated_data_v2.py
```

### 4. 検証
- 出力されたdata-integrated.jsonのサイズと内容を確認
- 自治体数が415になっているか確認
- 段落数が40,158になっているか確認
- JSONの構造が正しいか確認

### 5. clause-viewerでの動作確認
- ブラウザでviewer-advanced.htmlを開く
- データが正しく表示されるか確認
- フィルタリング機能が正常に動作するか確認

## 注意事項
- h5は1から始まる連番で、main4.3.csvの行番号(ヘッダーを除く)と対応
- munic._coding.v.4.csvのコーディング項目は値が1の場合のみそのコードが適用される
- result_solar_rule_v1.1.csvには314自治体のデータしかないため、一部の自治体はmunicipal_infoに含まれない可能性がある
- ファイルサイズが大きくなる(現在74,695行→推定100,000行以上)ため、JSONの圧縮や最適化を検討

## 代替案: SQLデータベースの利用

### SQLを使うメリット
1. **更新の容易性**
   - 新しいCSVデータが来たら、テーブルに INSERT/UPDATE するだけ
   - データの部分更新が簡単
   - バージョン管理がしやすい

2. **クエリの柔軟性**
   - 自治体ごと、コード種別ごとのフィルタリングが高速
   - 集計・分析クエリが書きやすい
   - JOINで必要なデータだけを取得可能

3. **パフォーマンス**
   - インデックスによる高速検索
   - 大量データでもメモリ効率が良い
   - JSONファイル全体を読み込む必要がない

4. **保守性**
   - データとロジックの分離
   - スキーマ定義で型チェック
   - データの整合性を保証

### 推奨DB構造
```sql
-- 自治体マスタ
CREATE TABLE municipalities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    name_eng TEXT,
    cases_count INTEGER,
    regulation_type TEXT,
    area_type TEXT,
    prohibited_area_ratio REAL,
    strictness_absolute TEXT,
    strictness_relative TEXT,
    process_emphasis TEXT,
    strictness_score REAL,
    participation_score REAL,
    procedure_score REAL
);

-- コーディング種別マスタ
CREATE TABLE coding_types (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    code TEXT NOT NULL UNIQUE,
    description TEXT
);

-- 段落データ
CREATE TABLE paragraphs (
    h5 INTEGER PRIMARY KEY,
    municipality_id INTEGER,
    year TEXT,
    category TEXT,  -- 区分(条例/規則)
    dan INTEGER,
    text TEXT,
    FOREIGN KEY (municipality_id) REFERENCES municipalities(id)
);

-- コーディング詳細(正規化版)
CREATE TABLE paragraph_codings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    paragraph_id INTEGER,
    coding_type_id INTEGER,
    FOREIGN KEY (paragraph_id) REFERENCES paragraphs(h5),
    FOREIGN KEY (coding_type_id) REFERENCES coding_types(id),
    UNIQUE(paragraph_id, coding_type_id)
);

-- インデックス
CREATE INDEX idx_paragraphs_municipality ON paragraphs(municipality_id);
CREATE INDEX idx_paragraph_codings_paragraph ON paragraph_codings(paragraph_id);
CREATE INDEX idx_paragraph_codings_coding ON paragraph_codings(coding_type_id);
```

### Web表示用のアプローチ
#### オプション1: API + フロントエンド
```python
# Flask/FastAPI等でAPIを作成
@app.get("/api/municipalities")
def get_municipalities():
    return db.query("SELECT * FROM municipalities")

@app.get("/api/paragraphs/{municipality_id}")
def get_paragraphs(municipality_id: int):
    return db.query("""
        SELECT p.*, GROUP_CONCAT(ct.code) as codes
        FROM paragraphs p
        LEFT JOIN paragraph_codings pc ON p.h5 = pc.paragraph_id
        LEFT JOIN coding_types ct ON pc.coding_type_id = ct.id
        WHERE p.municipality_id = ?
        GROUP BY p.h5
    """, municipality_id)
```

#### オプション2: 静的JSON生成(現行維持)
```python
# データ更新時のみSQLからJSONを生成
def export_to_json():
    # SQLiteから必要なデータを取得してJSONに変換
    # clause-viewerはそのまま使える
```

### 実装の選択肢

#### A案: 完全SQL移行(推奨)
- SQLite/PostgreSQLでDB構築
- WebサーバーでAPI提供
- フロントエンドから動的にデータ取得
- **メリット**: 最も保守性が高い、データ更新が簡単
- **デメリット**: サーバー環境が必要、実装工数大

#### B案: ハイブリッド
- SQLiteでデータ管理
- 表示用JSONは自動生成スクリプトで作成
- clause-viewerは現行のまま
- **メリット**: 実装が比較的簡単、静的ホスティング可能
- **デメリット**: JSONファイルサイズの問題は残る

#### C案: 現行維持 + 改善
- CSV→JSON変換スクリプトを改善
- データの増分更新ロジックを追加
- **メリット**: 最小限の変更
- **デメリット**: スケーラビリティに限界

### 推奨: B案(ハイブリッド)
1. SQLiteでデータ管理(ローカル)
2. データ更新スクリプトでCSV→SQLiteに投入
3. 表示用にSQLite→JSON変換
4. 必要に応じてA案(フルAPI)に移行

```bash
# 実装例
python3 import_csv_to_sqlite.py  # CSV→SQLite
python3 export_sqlite_to_json.py  # SQLite→JSON(clause-viewer用)
```

---

## B案実装手順書

### フェーズ1: SQLiteデータベースの構築

#### 1.1 データベース設計の実装
**ファイル**: `clause-viewer/setup_database.py`

```python
import sqlite3
from pathlib import Path

def setup_database():
    """データベースとテーブルを作成"""
    db_path = Path('clause-viewer/clause_data.db')
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # テーブル作成SQL実行
    # (上記のDB構造を実装)
    
    conn.commit()
    conn.close()
```

**実行**:
```bash
cd /home/ubuntu/cur/isep
python3 clause-viewer/setup_database.py
```

#### 1.2 CSVデータのインポート
**ファイル**: `clause-viewer/import_csv_to_sqlite.py`

**処理フロー**:
1. `result_solar_rule_v1.1.csv` → `municipalities`テーブル
   - 自治体名をキーにINSERT OR REPLACE
   
2. コーディング種別 → `coding_types`テーブル
   - munic._coding.v.4.csvのヘッダーから取得
   - 33種類のコードをマスタ登録

3. `main4.3.csv` + `munic._coding.v.4.csv` → `paragraphs` + `paragraph_codings`
   - h5をキーに両CSVを結合
   - トランザクション処理で一括投入
   - 進捗表示(40,158行を処理)

**実行**:
```bash
cd /home/ubuntu/cur/isep
python3 clause-viewer/import_csv_to_sqlite.py
```

**期待される出力**:
```
データインポート開始...
[1/4] 自治体マスタをインポート中...
  ✓ 314自治体を登録
[2/4] コーディング種別をインポート中...
  ✓ 33種類を登録
[3/4] 段落データをインポート中...
  進捗: 40158/40158 (100%)
  ✓ 40,158段落を登録
[4/4] コーディング詳細をインポート中...
  進捗: 40158/40158 (100%)
  ✓ XXX,XXX件のコーディングを登録
完了! データベース: clause-viewer/clause_data.db (約XX MB)
```

### フェーズ2: JSON生成スクリプトの作成

#### 2.1 SQLite→JSON変換スクリプト
**ファイル**: `clause-viewer/export_sqlite_to_json.py`

**処理内容**:
```python
def export_to_json():
    """SQLiteから data-integrated.json を生成"""
    
    # 1. 自治体リスト取得
    municipalities = query("SELECT name FROM municipalities ORDER BY name")
    
    # 2. コーディング種別リスト取得
    coding_types = query("SELECT code FROM coding_types ORDER BY code")
    
    # 3. 自治体情報取得
    municipality_info = query("""
        SELECT name, cases_count, regulation_type, area_type,
               prohibited_area_ratio, strictness_absolute, 
               strictness_relative, process_emphasis,
               strictness_score, participation_score, procedure_score
        FROM municipalities
    """)
    
    # 4. 段落データ取得(最適化: 一括取得してメモリで処理)
    paragraphs = query("""
        SELECT 
            p.h5,
            m.name as municipality,
            p.year,
            p.category,
            p.dan,
            p.text,
            GROUP_CONCAT(ct.code) as codes
        FROM paragraphs p
        INNER JOIN municipalities m ON p.municipality_id = m.id
        LEFT JOIN paragraph_codings pc ON p.h5 = pc.paragraph_id
        LEFT JOIN coding_types ct ON pc.coding_type_id = ct.id
        GROUP BY p.h5
        ORDER BY p.h5
    """)
    
    # 5. 統計情報計算
    statistics = calculate_statistics()
    
    # 6. JSON構造を構築
    data = {
        "municipalities": municipalities,
        "coding_types": coding_types,
        "municipality_info": municipality_info,
        "paragraphs": paragraphs,
        "statistics": statistics,
        "metadata": {
            "version": "2.0",
            "created_at": datetime.now().isoformat(),
            "source": "SQLite (clause_data.db)",
            "total_municipalities": len(municipalities),
            "total_paragraphs": len(paragraphs)
        }
    }
    
    # 7. JSONファイル出力
    with open('clause-viewer/data-integrated.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

**実行**:
```bash
cd /home/ubuntu/cur/isep
python3 clause-viewer/export_sqlite_to_json.py
```

**期待される出力**:
```
JSON生成開始...
[1/5] 自治体リスト取得...
  ✓ 415自治体
[2/5] コーディング種別取得...
  ✓ 33種類
[3/5] 自治体情報取得...
  ✓ 314自治体(分析済み)
[4/5] 段落データ取得...
  進捗: 40158/40158 (100%)
  ✓ 40,158段落
[5/5] JSON出力...
  ✓ ファイルサイズ: XX.X MB
完了! clause-viewer/data-integrated.json
```

### フェーズ3: 統合スクリプトとメンテナンス

#### 3.1 ワンコマンド更新スクリプト
**ファイル**: `clause-viewer/update_data.py`

```python
"""
データ更新の統合スクリプト
新しいCSVファイルが来たときに実行
"""

def main():
    print("=" * 60)
    print("データ更新プロセス")
    print("=" * 60)
    
    # ステップ1: データベースの初期化確認
    if not db_exists():
        print("\n[初回] データベースをセットアップ...")
        setup_database()
    
    # ステップ2: CSVインポート
    print("\n[1/2] CSVからSQLiteへインポート...")
    import_csv_to_sqlite()
    
    # ステップ3: JSON生成
    print("\n[2/2] SQLiteからJSON生成...")
    export_sqlite_to_json()
    
    print("\n" + "=" * 60)
    print("完了! clause-viewer/data-integrated.json を更新しました")
    print("=" * 60)

if __name__ == '__main__':
    main()
```

**実行**:
```bash
cd /home/ubuntu/cur/isep
python3 clause-viewer/update_data.py
```

#### 3.2 データ検証スクリプト
**ファイル**: `clause-viewer/validate_data.py`

```python
"""
データの整合性をチェック
"""

def validate():
    # 自治体数の一致確認
    # 段落数の一致確認
    # コーディング数の妥当性確認
    # JSON構造の妥当性確認
```

#### 3.3 差分更新スクリプト(オプション)
**ファイル**: `clause-viewer/update_incremental.py`

```python
"""
新しい自治体データのみを追加更新
"""

def incremental_update(new_csv_path):
    # 既存データとの差分を検出
    # 新規データのみINSERT
    # 既存データはUPDATE
```

### フェーズ4: ドキュメント作成

#### 4.1 README.md
**ファイル**: `clause-viewer/README.md`

```markdown
# Clause Viewer データ管理システム

## データ更新手順

### 通常の更新(全データ再生成)
```bash
python3 update_data.py
```

### 個別実行
```bash
# 1. データベースセットアップ(初回のみ)
python3 setup_database.py

# 2. CSVインポート
python3 import_csv_to_sqlite.py

# 3. JSON生成
python3 export_sqlite_to_json.py

# 4. データ検証
python3 validate_data.py
```

## ファイル構成
- `clause_data.db`: SQLiteデータベース
- `data-integrated.json`: viewer用JSON(自動生成)
- `*.py`: データ処理スクリプト

## データソース
- `/home/ubuntu/cur/isep/main4.3.csv`
- `/home/ubuntu/cur/isep/munic._coding.v.4.csv`
- `/home/ubuntu/cur/isep/result_solar_rule_v1.1.csv`
```

### 実装チェックリスト

- [ ] **Phase 1**: データベース構築
  - [ ] setup_database.py 作成
  - [ ] import_csv_to_sqlite.py 作成
  - [ ] データベース動作確認
  
- [ ] **Phase 2**: JSON生成
  - [ ] export_sqlite_to_json.py 作成
  - [ ] JSON構造の検証
  - [ ] clause-viewerでの表示確認
  
- [ ] **Phase 3**: 統合とメンテナンス
  - [ ] update_data.py 作成
  - [ ] validate_data.py 作成
  - [ ] エラーハンドリング実装
  
- [ ] **Phase 4**: ドキュメント
  - [ ] README.md 作成
  - [ ] 使用方法の文書化
  
- [ ] **Phase 5**: テストと最適化
  - [ ] 全データでの動作確認
  - [ ] パフォーマンステスト
  - [ ] ファイルサイズ確認

### 見積もり工数
- Phase 1: 2-3時間
- Phase 2: 2-3時間
- Phase 3: 1-2時間
- Phase 4: 1時間
- Phase 5: 1-2時間
- **合計**: 7-11時間

### 次のステップ
1. Phase 1から順に実装開始
2. 各フェーズ完了後に動作確認
3. 問題があれば修正・調整
4. 最終的にclause-viewerで表示確認 
